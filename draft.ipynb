{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596addaf8c6e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "import fitz\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_text_and_tables_correctly(pdf_path):\n",
    "    \"\"\"Extracts text while ensuring tables are inserted in the correct position without duplication.\"\"\"\n",
    "    final_output = []  \n",
    "\n",
    "    with fitz.open(pdf_path) as doc, pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(len(doc)):\n",
    "            text_blocks = doc[page_num].get_text(\"blocks\")  \n",
    "            text_blocks.sort(key=lambda x: (x[1], x[0])) \n",
    "\n",
    "           \n",
    "            table = pdf.pages[page_num].extract_table()\n",
    "\n",
    "            final_text = []\n",
    "            table_inserted = False\n",
    "            table_text_set = set()  \n",
    "\n",
    "            if table:\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])  \n",
    "                table_text = df.to_string(index=False)\n",
    "                table_text_lines = set(table_text.split(\"\\n\")) \n",
    "\n",
    "            for block in text_blocks:\n",
    "                block_text = block[4].strip()\n",
    "\n",
    "                \n",
    "                if table and not table_inserted:\n",
    "                    final_text.append(\"\\n[Extracted Table]\\n\" + table_text)\n",
    "                    table_inserted = True\n",
    "\n",
    "               \n",
    "                if block_text in table_text_lines:\n",
    "                    continue\n",
    "\n",
    "                final_text.append(block_text)  \n",
    "            \n",
    "            final_output.append(\"\\n\".join(final_text))\n",
    "\n",
    "    return \"\\n\".join(final_output)  \n",
    "\n",
    "# IN PROGRESS(NOT FULLY COMPLETED)\n",
    "def is_relevant_image(image):\n",
    "    \"\"\"Filters out table-like and text-heavy images, keeping only graphs and charts.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If an image has many structured boxes, it's likely a table\n",
    "    table_like_contours = [cnt for cnt in contours if cv2.boundingRect(cnt)[2] > 100 and cv2.boundingRect(cnt)[3] > 20]\n",
    "    if len(table_like_contours) > 10:\n",
    "        return False  # Likely a table\n",
    "\n",
    "    # Use OCR to check for too much text, which indicates a document or table\n",
    "    text = pytesseract.image_to_string(gray)\n",
    "    if len(text.strip()) > 50:\n",
    "        return False  # Too much text, likely not a chart\n",
    "\n",
    "    return True  # Likely a graph or chart\n",
    "\n",
    "# IN PROGRESS(NOT FULLY COMPLETED)\n",
    "def extract_graphs_and_charts(pdf_path, output_folder=\"extracted_graphs\"):\n",
    "    \"\"\"Extracts and saves only graphs, pie charts, and relevant images.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        img_path = os.path.join(output_folder, f\"page_{i+1}.png\")\n",
    "\n",
    "        # Convert to OpenCV format\n",
    "        open_cv_image = np.array(image)\n",
    "        open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if is_relevant_image(open_cv_image):\n",
    "            image.save(img_path, \"PNG\")\n",
    "            image_paths.append(img_path)\n",
    "            print(f\"\\n[Graph/Chart from Page {i+1}]\")\n",
    "            display(image)  # Show the image\n",
    "\n",
    "\n",
    "def parse_financial_data(text):\n",
    "    revenue = re.findall(r\"Revenues\\s*[\\$]\\s*([\\d,]+)\", text)\n",
    "    operating_income = re.findall(r\"Operating income\\s*[\\$]\\s*([\\d,]+)\", text)\n",
    "    net_income = re.findall(r\"Net income\\s*[\\$]\\s*([\\d,]+)\", text)\n",
    "    print(\"Extracted Revenues:\", revenue)\n",
    "    print(\"Extracted Operating Income:\", operating_income)\n",
    "    print(\"Extracted Net Income:\", net_income)\n",
    "    \n",
    "    financial_data = {\n",
    "        \"revenue\": revenue,\n",
    "        \"operating_income\": operating_income,\n",
    "        \"net_income\": net_income\n",
    "    }\n",
    "    return financial_data\n",
    "\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        max_chunk_size = 1000\n",
    "        text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "        \n",
    "        summary = \"\"\n",
    "        for chunk in text_chunks:\n",
    "            chunk_summary = summarizer(chunk, max_length=200, min_length=50, do_sample=False)\n",
    "            if chunk_summary and isinstance(chunk_summary, list) and \"summary_text\" in chunk_summary[0]:\n",
    "                summary += chunk_summary[0][\"summary_text\"] + \" \"\n",
    "            else:\n",
    "                print(\"Warning: Summarization did not return any results for a chunk.\")\n",
    "                \n",
    "        if summary.strip() == \"\":\n",
    "            return \"No summary available.\"\n",
    "        \n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return \"Summarization failed.\"\n",
    "def plot_financial_trends(financial_data):\n",
    "    try:\n",
    "        if financial_data[\"revenue\"]:\n",
    "            revenue_data = [int(value.replace(\",\", \"\")) for value in financial_data[\"revenue\"]]\n",
    "        else:\n",
    "            print(\"No revenue data found to plot.\")\n",
    "            revenue_data = []\n",
    "        \n",
    "        if financial_data[\"operating_income\"]:\n",
    "            operating_income_data = [int(value.replace(\",\", \"\")) for value in financial_data[\"operating_income\"]]\n",
    "        else:\n",
    "            print(\"No operating income data found to plot.\")\n",
    "            operating_income_data = []\n",
    "        \n",
    "        min_length = min(len(revenue_data), len(operating_income_data))\n",
    "        revenue_data = revenue_data[:min_length]\n",
    "        operating_income_data = operating_income_data[:min_length]\n",
    "        if not revenue_data or not operating_income_data:\n",
    "            print(\"Insufficient data for plotting financial trends.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(revenue_data, marker='o', label=\"Revenue\")\n",
    "        plt.plot(operating_income_data, marker='s', label=\"Operating Income\")\n",
    "        plt.legend()\n",
    "        plt.title('Financial Trends (Revenue vs Operating Income)')\n",
    "        plt.xlabel('Data Points (e.g., Quarters)')\n",
    "        plt.ylabel('Amount ($ millions)')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error while plotting financial trends: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\Aaarat\\Downloads\\2024q1-alphabet-earnings-release-pdf.pdf\"\n",
    "    pdf_text = extract_text_and_tables_correctly(pdf_path)\n",
    "    \n",
    "    if pdf_text:\n",
    "        financial_data = parse_financial_data(pdf_text)\n",
    "        print(\"Financial Data Extracted:\", financial_data)\n",
    "        summary = summarize_text(pdf_text)\n",
    "        print(\"Document Summary:\", summary)\n",
    "        plot_financial_trends(financial_data)\n",
    "    else:\n",
    "        print(\"No text extracted from PDF; please check the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e806bfe12a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        processed_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "processed_sentences = preprocess_text(text)\n",
    "print(processed_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385eb0a74e4d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "labels = [\"Strength\", \"Weakness\", \"Opportunity\", \"Threat\"]\n",
    "swot_results = {\"Strengths\": [], \"Weaknesses\": [], \"Opportunities\": [], \"Threats\": []}\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    result = classifier(sentence, candidate_labels=labels)\n",
    "    label = result['labels'][0]\n",
    "    if label == \"Strength\":\n",
    "        swot_results[\"Strengths\"].append(sentence)\n",
    "    elif label == \"Weakness\":\n",
    "        swot_results[\"Weaknesses\"].append(sentence)\n",
    "    elif label == \"Opportunity\":\n",
    "        swot_results[\"Opportunities\"].append(sentence)\n",
    "    elif label == \"Threat\":\n",
    "        swot_results[\"Threats\"].append(sentence)\n",
    "\n",
    "print(swot_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d6a68d5268705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_swot_results(swot_results):\n",
    "    for category, sentences in swot_results.items():\n",
    "        print(f\"\\n{category}:\\n\")\n",
    "        for sentence in sentences:\n",
    "            print(f\"- {sentence}\")\n",
    "\n",
    "display_swot_results(swot_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
